# ============================================
# GESTION DES DOUBLONS ENTRE TRAIN ET TEST
# ============================================
import hashlib
from collections import defaultdict

def get_file_hash(file_path):
    """Calcule le hash MD5 d'un fichier"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        print(f"Erreur lors du calcul du hash pour {file_path}: {e}")
        return None

# Liste des √©motions (sous-dossiers)
EMOTIONS = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']

def get_file_hash_fast(file_path):
    """Calcule le hash MD5 d'un fichier (version optimis√©e avec chunks plus grands)"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(65536), b""):  # 64KB chunks (plus rapide)
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        return None

def find_duplicates(train_dir, test_dir, remove_from_test=True):
    """
    D√©tecte et supprime les doublons entre train et test (VERSION OPTIMIS√âE)
    
    IMPORTANT: Seuls les fichiers avec le M√äME CONTENU (m√™me hash) sont consid√©r√©s comme doublons.
    Les fichiers avec le m√™me nom mais contenu diff√©rent ne sont PAS supprim√©s.
    
    Strat√©gie d'optimisation:
    1. Filtre par taille de fichier (RAPIDE - r√©duit le nombre de candidats)
    2. V√©rifie par hash MD5 (PR√âCIS - seule m√©thode de confirmation)
    
    Args:
        train_dir: Dossier d'entra√Ænement
        test_dir: Dossier de test
        remove_from_test: Si True, supprime les doublons du test (recommand√©)
    """
    print("üîç Recherche des vrais doublons entre train et test...")
    print("   ‚ö†Ô∏è  Seuls les fichiers avec le M√äME CONTENU seront supprim√©s")
    print("   (M√©thode: taille ‚Üí hash MD5 pour confirmation)")
    
    train_path = Path(train_dir)
    test_path = Path(test_dir)
    
    if not train_path.exists():
        print(f"‚ùå Dossier train introuvable: {train_dir}")
        return []
    if not test_path.exists():
        print(f"‚ùå Dossier test introuvable: {test_dir}")
        return []
    
    # √âtape 1: Collecter tous les fichiers par nom (pour statistiques)
    print("\n‚ö° √âtape 1: Collecte des fichiers...")
    train_files_by_name = defaultdict(list)
    test_files_by_name = defaultdict(list)
    
    for emotion in EMOTIONS:
        train_emotion_dir = train_path / emotion
        test_emotion_dir = test_path / emotion
        
        if train_emotion_dir.exists():
            for img_file in list(train_emotion_dir.glob("*.jpg")) + list(train_emotion_dir.glob("*.png")):
                train_files_by_name[img_file.name].append(img_file)
        
        if test_emotion_dir.exists():
            for img_file in list(test_emotion_dir.glob("*.jpg")) + list(test_emotion_dir.glob("*.png")):
                test_files_by_name[img_file.name].append(img_file)
    
    # Statistiques sur les noms similaires (information uniquement)
    potential_by_name = 0
    for filename in test_files_by_name:
        if filename in train_files_by_name:
            potential_by_name += len(test_files_by_name[filename]) * len(train_files_by_name[filename])
    
    print(f"   üìã {potential_by_name} paires avec m√™me nom (√† v√©rifier par hash)")
    
    # √âtape 2: Comparaison par taille pour optimiser (filtre rapide)
    print("\n‚ö° √âtape 2: Filtrage par taille de fichier...")
    
    # Grouper tous les fichiers par taille
    train_files_by_size = defaultdict(list)
    test_files_by_size = defaultdict(list)
    
    for files_list in train_files_by_name.values():
        for img_file in files_list:
            try:
                size = img_file.stat().st_size
                train_files_by_size[size].append(img_file)
            except:
                pass
    
    for files_list in test_files_by_name.values():
        for img_file in files_list:
            try:
                size = img_file.stat().st_size
                test_files_by_size[size].append(img_file)
            except:
                pass
    
    # Candidats pour v√©rification par hash (m√™me taille)
    candidates_for_hash = []
    for size in test_files_by_size:
        if size in train_files_by_size:
            for test_file in test_files_by_size[size]:
                for train_file in train_files_by_size[size]:
                    candidates_for_hash.append((train_file, test_file))
    
    print(f"   üìã {len(candidates_for_hash)} candidats √† v√©rifier par hash (m√™me taille)...")
    
    # √âtape 3: V√©rification par hash (SEULE M√âTHODE DE CONFIRMATION)
    # On v√©rifie par hash TOUS les candidats, m√™me ceux avec le m√™me nom
    # car les noms peuvent se dupliquer sans que les fichiers soient identiques
    print("\n‚ö° √âtape 3: V√©rification par hash (confirmation des vrais doublons)...")
    print("   ‚ö†Ô∏è  Seuls les fichiers avec le M√äME CONTENU seront consid√©r√©s comme doublons")
    
    duplicates_confirmed = []  # Seuls les vrais doublons (m√™me hash)
    checked = 0
    
    for train_file, test_file in candidates_for_hash:
        checked += 1
        if checked % 1000 == 0:
            print(f"   Progression: {checked}/{len(candidates_for_hash)} v√©rifi√©s...")
        
        train_hash = get_file_hash_fast(train_file)
        test_hash = get_file_hash_fast(test_file)
        
        if train_hash and test_hash and train_hash == test_hash:
            duplicates_confirmed.append({
                'train_file': train_file,
                'test_file': test_file,
                'hash': train_hash
            })
    
    print(f"   ‚úÖ {len(duplicates_confirmed)} vrais doublons confirm√©s (m√™me contenu)")
    
    total_train = sum(len(files) for files in train_files_by_name.values())
    total_test = sum(len(files) for files in test_files_by_name.values())
    
    print(f"\nüìä Statistiques finales:")
    print(f"   - Fichiers dans train: {total_train}")
    print(f"   - Fichiers dans test: {total_test}")
    print(f"   - Paires avec m√™me nom: {potential_by_name} (information)")
    print(f"   - Vrais doublons confirm√©s (m√™me contenu): {len(duplicates_confirmed)}")
    
    if len(duplicates_confirmed) == 0:
        print("\n‚úÖ Aucun doublon d√©tect√© (fichiers avec contenu identique)!")
        return []
    
    # Afficher quelques exemples
    print(f"\nüìã Exemples de vrais doublons (premiers 5):")
    for i, dup in enumerate(duplicates_confirmed[:5]):
        print(f"   {i+1}. Train: {dup['train_file'].name} | Test: {dup['test_file'].name}")
        print(f"       Hash: {dup['hash'][:16]}...")
    
    if len(duplicates_confirmed) > 5:
        print(f"   ... et {len(duplicates_confirmed) - 5} autres")
    
    # Supprimer UNIQUEMENT les vrais doublons (confirm√©s par hash)
    if remove_from_test:
        print(f"\nüóëÔ∏è  Suppression des {len(duplicates_confirmed)} vrais doublons du dossier test...")
        print("   (Seuls les fichiers avec le M√äME CONTENU sont supprim√©s)")
        removed_count = 0
        removed_files = set()  # Pour √©viter les suppressions multiples
        
        for dup in duplicates_confirmed:
            test_file = dup['test_file']
            # V√©rifier que le fichier existe et n'a pas d√©j√† √©t√© supprim√©
            if test_file.exists() and str(test_file) not in removed_files:
                try:
                    test_file.unlink()  # Supprime le fichier
                    removed_files.add(str(test_file))
                    removed_count += 1
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Erreur lors de la suppression de {test_file.name}: {e}")
        
        print(f"‚úÖ {removed_count} fichiers supprim√©s du test")
        print(f"   - Fichiers restants dans test: {total_test - removed_count}")
    else:
        print("\n‚ö†Ô∏è Suppression d√©sactiv√©e. Les doublons sont conserv√©s.")
        print("   Pour supprimer, ex√©cutez: find_duplicates(train_dir, test_dir, remove_from_test=True)")
    
    return duplicates_confirmed

# Ex√©cuter la d√©tection et suppression des doublons
# D√©commentez la ligne suivante pour activer la suppression automatique
# find_duplicates(train_dir, test_dir, remove_from_test=True)

Ex√©cuter la d√©tection RAPIDE des doublons (sans suppression)
print("=" * 60)
print("D√âTECTION RAPIDE DES DOUBLONS (Mode analyse uniquement)")
print("=" * 60)
duplicates = find_duplicates(train_dir, test_dir, remove_from_test=True)

Pour supprimer les doublons, d√©commentez la ligne suivante:
find_duplicates(train_dir, test_dir, remove_from_test=True)



# ============================================
# FONCTION SIMPLIFI√âE ET SYNTH√âTIS√âE
# ============================================
import hashlib
from collections import defaultdict
import json
from datetime import datetime

def find_duplicates_simple(train_dir, test_dir, remove_duplicates=False, remove_from='test'):
    """
    Fonction SIMPLIFI√âE pour d√©tecter et supprimer les doublons entre train et test
    
    IMPORTANT: 
    - Prend 100% des fichiers du dossier test (pas de pourcentage)
    - V√©rifie les doublons entre test et train
    - Code synth√©tis√© et simplifi√©
    
    Args:
        train_dir: Dossier d'entra√Ænement
        test_dir: Dossier de test
        remove_duplicates: Si True, supprime les doublons du test
        remove_from: 'test' (supprime du test) ou 'train' (supprime du train)
    """
    print("=" * 80)
    print("üîç D√âTECTION DES DOUBLONS (100% des fichiers test)")
    print("=" * 80)
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    train_path = Path(train_dir).resolve()
    test_path = Path(test_dir).resolve()
    
    if not train_path.exists() or not test_path.exists():
        print(f"‚ùå Dossiers introuvables")
        return []
    
    # ============================================
    # COLLECTE ET CALCUL DES HASH (SYNTH√âTIS√â)
    # ============================================
    print("üìÇ Collecte des fichiers et calcul des hash...")
    
    train_hash_to_files = defaultdict(list)
    test_hash_to_files = defaultdict(list)
    
    # Train
    train_count = 0
    for emotion in EMOTIONS:
        emotion_dir = train_path / emotion
        if emotion_dir.exists():
            for img_file in list(emotion_dir.glob("*.jpg")) + list(emotion_dir.glob("*.png")):
                file_hash = get_file_hash_fast(img_file)
                if file_hash:
                    train_hash_to_files[file_hash].append(img_file.resolve())
                train_count += 1
                if train_count % 1000 == 0:
                    print(f"   Train: {train_count} fichiers...")
    
    # Test (100% - TOUS les fichiers)
    test_count = 0
    for emotion in EMOTIONS:
        emotion_dir = test_path / emotion
        if emotion_dir.exists():
            for img_file in list(emotion_dir.glob("*.jpg")) + list(emotion_dir.glob("*.png")):
                file_hash = get_file_hash_fast(img_file)
                if file_hash:
                    test_hash_to_files[file_hash].append(img_file.resolve())
                test_count += 1
                if test_count % 1000 == 0:
                    print(f"   Test: {test_count} fichiers...")
    
    print(f"   ‚úÖ Train: {train_count} fichiers, {len(train_hash_to_files)} uniques")
    print(f"   ‚úÖ Test: {test_count} fichiers (100%), {len(test_hash_to_files)} uniques")
    
    # ============================================
    # D√âTECTION DES DOUBLONS (SYNTH√âTIS√â)
    # ============================================
    print("\nüîç D√©tection des doublons entre train et test...")
    
    duplicates = []
    for file_hash in test_hash_to_files:
        if file_hash in train_hash_to_files:
            # Fichier pr√©sent dans train ET test = doublon
            for test_file in test_hash_to_files[file_hash]:
                duplicates.append({
                    'hash': file_hash,
                    'train_file': train_hash_to_files[file_hash][0],
                    'test_file': test_file
                })
    
    print(f"   ‚úÖ {len(duplicates)} doublons trouv√©s")
    
    if len(duplicates) == 0:
        print("\n‚úÖ Aucun doublon d√©tect√©!")
        return []
    
    # Afficher quelques exemples
    print(f"\nüìã Exemples (premiers 5):")
    for i, dup in enumerate(duplicates[:5]):
        print(f"   {i+1}. {dup['test_file'].name}")
    
    # ============================================
    # SUPPRESSION (SIMPLIFI√âE)
    # ============================================
    if remove_duplicates:
        print(f"\nüóëÔ∏è  Suppression de {len(duplicates)} doublons...")
        
        def delete_file(file_path):
            """Supprime un fichier"""
            try:
                file_path = Path(file_path).resolve()
                if file_path.exists():
                    file_path.unlink()
                    return True
                return False
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erreur: {file_path.name} - {e}")
                return False
        
        removed = 0
        for dup in duplicates:
            file_to_delete = dup['test_file'] if remove_from == 'test' else dup['train_file']
            if delete_file(file_to_delete):
                removed += 1
        
        print(f"‚úÖ {removed} fichiers supprim√©s avec succ√®s")
    else:
        print("\n‚ö†Ô∏è Suppression d√©sactiv√©e. Mettez remove_duplicates=True pour supprimer")
    
    print("\n" + "=" * 80)
    return duplicates

# Ex√©cuter la d√©tection
# duplicates = find_duplicates_simple(train_dir, test_dir, remove_duplicates=False)


Ex√©cuter la d√©tection des doublons (version simplifi√©e)
=======================================================

D√©tection sans suppression
duplicates = find_duplicates_simple(train_dir, test_dir, remove_duplicates=False)

Pour supprimer les doublons, d√©commentez la ligne suivante:
duplicates = find_duplicates_simple(train_dir, test_dir, remove_duplicates=True, remove_from='test')

# ============================================
# FONCTION SIMPLIFI√âE DE D√âTECTION DE DOUBLONS
# ============================================
import hashlib
from collections import defaultdict
import json
from datetime import datetime

def find_all_duplicates_exhaustive(train_dir, test_dir, 
                                     check_train_internal=True,
                                     check_test_internal=True,
                                     check_cross=True,
                                     remove_duplicates=False,
                                     remove_from='test',
                                     export_report=False):
    """
    Fonction EXHAUSTIVE pour d√©tecter TOUS les doublons possibles
    
    D√©tecte:
    1. Doublons √† l'int√©rieur du dossier train
    2. Doublons √† l'int√©rieur du dossier test
    3. Doublons entre train et test
    
    Args:
        train_dir: Dossier d'entra√Ænement
        test_dir: Dossier de test
        check_train_internal: V√©rifier les doublons dans train
        check_test_internal: V√©rifier les doublons dans test
        check_cross: V√©rifier les doublons entre train et test
        remove_duplicates: Si True, supprime les doublons
        remove_from: 'test', 'train', ou 'both' - d'o√π supprimer
        export_report: Si True, exporte un rapport JSON
    
    Returns:
        dict: Rapport complet avec tous les doublons trouv√©s
    """
    print("=" * 80)
    print("üîç ANALYSE EXHAUSTIVE DES DOUBLONS")
    print("=" * 80)
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    train_path = Path(train_dir)
    test_path = Path(test_dir)
    
    if not train_path.exists():
        print(f"‚ùå Dossier train introuvable: {train_dir}")
        return None
    if not test_path.exists():
        print(f"‚ùå Dossier test introuvable: {test_dir}")
        return None
    
    # ============================================
    # √âTAPE 1: COLLECTE DE TOUS LES FICHIERS
    # ============================================
    print("üìÇ √âtape 1: Collecte de tous les fichiers...")
    
    train_files = {}  # {hash: [list of files]}
    test_files = {}   # {hash: [list of files]}
    
    # Collecter les fichiers train
    train_file_list = []
    for emotion in EMOTIONS:
        emotion_dir = train_path / emotion
        if emotion_dir.exists():
            for img_file in list(emotion_dir.glob("*.jpg")) + list(emotion_dir.glob("*.png")):
                train_file_list.append(img_file)
    
    # Collecter les fichiers test
    test_file_list = []
    for emotion in EMOTIONS:
        emotion_dir = test_path / emotion
        if emotion_dir.exists():
            for img_file in list(emotion_dir.glob("*.jpg")) + list(emotion_dir.glob("*.png")):
                test_file_list.append(img_file)
    
    print(f"   ‚úÖ {len(train_file_list)} fichiers dans train")
    print(f"   ‚úÖ {len(test_file_list)} fichiers dans test")
    
    # ============================================
    # √âTAPE 2: CALCUL DES HASH (avec progression)
    # ============================================
    print("\n‚ö° √âtape 2: Calcul des hash MD5...")
    
    def calculate_hashes_batch(file_list, label):
        """Calcule les hashs pour une liste de fichiers"""
        file_hash_map = {}  # {file_path: hash}
        hash_to_files = defaultdict(list)  # {hash: [files]}
        
        total = len(file_list)
        for i, img_file in enumerate(file_list):
            if (i + 1) % 500 == 0 or i == 0:
                print(f"   {label}: {i+1}/{total} fichiers trait√©s...")
            
            file_hash = get_file_hash_fast(img_file)
            if file_hash:
                file_hash_map[img_file] = file_hash
                hash_to_files[file_hash].append(img_file)
        
        print(f"   ‚úÖ {label}: {total} fichiers trait√©s")
        return file_hash_map, hash_to_files
    
    train_hash_map, train_hash_to_files = calculate_hashes_batch(train_file_list, "Train")
    test_hash_map, test_hash_to_files = calculate_hashes_batch(test_file_list, "Test")
    
    # ============================================
    # √âTAPE 3: D√âTECTION DES DOUBLONS
    # ============================================
    print("\nüîç √âtape 3: D√©tection des doublons...")
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'train_dir': str(train_dir),
        'test_dir': str(test_dir),
        'train_internal_duplicates': [],
        'test_internal_duplicates': [],
        'cross_duplicates': [],
        'statistics': {}
    }
    
    # 3.1 Doublons dans train (interne)
    if check_train_internal:
        print("\n   üìä Analyse des doublons dans TRAIN...")
        train_internal = []
        for file_hash, files in train_hash_to_files.items():
            if len(files) > 1:
                train_internal.append({
                    'hash': file_hash,
                    'count': len(files),
                    'files': [str(f) for f in files],
                    'emotions': [f.parent.name for f in files]
                })
        report['train_internal_duplicates'] = train_internal
        print(f"      ‚úÖ {len(train_internal)} groupes de doublons trouv√©s dans train")
        if train_internal:
            total_dups = sum(d['count'] - 1 for d in train_internal)
            print(f"      üìã {total_dups} fichiers en doublon (peuvent √™tre supprim√©s)")
    
    # 3.2 Doublons dans test (interne)
    if check_test_internal:
        print("\n   üìä Analyse des doublons dans TEST...")
        test_internal = []
        for file_hash, files in test_hash_to_files.items():
            if len(files) > 1:
                test_internal.append({
                    'hash': file_hash,
                    'count': len(files),
                    'files': [str(f) for f in files],
                    'emotions': [f.parent.name for f in files]
                })
        report['test_internal_duplicates'] = test_internal
        print(f"      ‚úÖ {len(test_internal)} groupes de doublons trouv√©s dans test")
        if test_internal:
            total_dups = sum(d['count'] - 1 for d in test_internal)
            print(f"      üìã {total_dups} fichiers en doublon (peuvent √™tre supprim√©s)")
    
    # 3.3 Doublons entre train et test
    if check_cross:
        print("\n   üìä Analyse des doublons entre TRAIN et TEST...")
        cross_duplicates = []
        for file_hash in train_hash_to_files:
            if file_hash in test_hash_to_files:
                train_files_list = train_hash_to_files[file_hash]
                test_files_list = test_hash_to_files[file_hash]
                
                for test_file in test_files_list:
                    cross_duplicates.append({
                        'hash': file_hash,
                        'train_file': str(train_files_list[0]),  # Prendre le premier
                        'test_file': str(test_file),
                        'train_emotion': train_files_list[0].parent.name,
                        'test_emotion': test_file.parent.name
                    })
        
        report['cross_duplicates'] = cross_duplicates
        print(f"      ‚úÖ {len(cross_duplicates)} doublons trouv√©s entre train et test")
    
    # ============================================
    # √âTAPE 4: STATISTIQUES D√âTAILL√âES
    # ============================================
    print("\nüìä √âtape 4: G√©n√©ration des statistiques...")
    
    stats = {
        'total_train_files': len(train_file_list),
        'total_test_files': len(test_file_list),
        'unique_train_files': len(train_hash_to_files),
        'unique_test_files': len(test_hash_to_files),
        'train_internal': {
            'duplicate_groups': len(report['train_internal_duplicates']),
            'duplicate_files': sum(d['count'] - 1 for d in report['train_internal_duplicates']),
            'space_wasted_mb': 0  # Sera calcul√©
        },
        'test_internal': {
            'duplicate_groups': len(report['test_internal_duplicates']),
            'duplicate_files': sum(d['count'] - 1 for d in report['test_internal_duplicates']),
            'space_wasted_mb': 0
        },
        'cross_duplicates': len(report['cross_duplicates'])
    }
    
    # Calculer l'espace gaspill√©
    def calculate_wasted_space(duplicates_list):
        total_size = 0
        for dup_group in duplicates_list:
            if dup_group['files']:
                try:
                    file_size = Path(dup_group['files'][0]).stat().st_size
                    # Compter les fichiers en doublon (count - 1)
                    total_size += file_size * (dup_group['count'] - 1)
                except:
                    pass
        return total_size / (1024 * 1024)  # En MB
    
    stats['train_internal']['space_wasted_mb'] = calculate_wasted_space(report['train_internal_duplicates'])
    stats['test_internal']['space_wasted_mb'] = calculate_wasted_space(report['test_internal_duplicates'])
    
    report['statistics'] = stats
    
    # ============================================
    # √âTAPE 5: AFFICHAGE DU RAPPORT
    # ============================================
    print("\n" + "=" * 80)
    print("üìä RAPPORT COMPLET DES DOUBLONS")
    print("=" * 80)
    print(f"\nüìÅ Fichiers:")
    print(f"   - Train: {stats['total_train_files']} fichiers ({stats['unique_train_files']} uniques)")
    print(f"   - Test: {stats['total_test_files']} fichiers ({stats['unique_test_files']} uniques)")
    
    if check_train_internal:
        print(f"\nüîç Doublons dans TRAIN:")
        print(f"   - Groupes de doublons: {stats['train_internal']['duplicate_groups']}")
        print(f"   - Fichiers en doublon: {stats['train_internal']['duplicate_files']}")
        print(f"   - Espace gaspill√©: {stats['train_internal']['space_wasted_mb']:.2f} MB")
    
    if check_test_internal:
        print(f"\nüîç Doublons dans TEST:")
        print(f"   - Groupes de doublons: {stats['test_internal']['duplicate_groups']}")
        print(f"   - Fichiers en doublon: {stats['test_internal']['duplicate_files']}")
        print(f"   - Espace gaspill√©: {stats['test_internal']['space_wasted_mb']:.2f} MB")
    
    if check_cross:
        print(f"\nüîç Doublons entre TRAIN et TEST:")
        print(f"   - Nombre de doublons: {stats['cross_duplicates']}")
        if stats['cross_duplicates'] > 0:
            # Afficher quelques exemples
            print(f"\n   Exemples (premiers 5):")
            for i, dup in enumerate(report['cross_duplicates'][:5]):
                print(f"      {i+1}. Train: {Path(dup['train_file']).name} ({dup['train_emotion']})")
                print(f"         Test: {Path(dup['test_file']).name} ({dup['test_emotion']})")
    
    # ============================================
    # √âTAPE 6: SUPPRESSION (si demand√©e) - VERSION SIMPLIFI√âE
    # ============================================
    if remove_duplicates:
        print("\n" + "=" * 80)
        print("üóëÔ∏è  SUPPRESSION DES DOUBLONS")
        print("=" * 80)
        
        def delete_file(file_path_str):
            """Fonction simple pour supprimer un fichier"""
            try:
                # Convertir en Path et r√©soudre le chemin absolu
                file_path = Path(file_path_str).resolve()
                
                if not file_path.exists():
                    return False, f"Fichier introuvable: {file_path}"
                
                # Supprimer le fichier
                file_path.unlink()
                
                # V√©rifier que la suppression a r√©ussi
                if file_path.exists():
                    return False, f"√âchec de la suppression: {file_path}"
                
                return True, "OK"
            except Exception as e:
                return False, f"Erreur: {e}"
        
        total_removed = 0
        
        # 1. Supprimer les doublons cross (train/test) du TEST
        if check_cross and remove_from in ['test', 'both']:
            print(f"\nüóëÔ∏è  Suppression des doublons cross du TEST...")
            print(f"   {len(report['cross_duplicates'])} fichiers √† supprimer")
            
            count = 0
            for dup in report['cross_duplicates']:
                success, msg = delete_file(dup['test_file'])
                if success:
                    count += 1
                    total_removed += 1
                elif "introuvable" not in msg.lower():
                    print(f"   ‚ö†Ô∏è {Path(dup['test_file']).name}: {msg}")
            
            print(f"   ‚úÖ {count} fichiers supprim√©s")
        
        # 2. Supprimer les doublons internes du TEST
        if check_test_internal and remove_from in ['test', 'both']:
            print(f"\nüóëÔ∏è  Suppression des doublons internes du TEST...")
            
            count = 0
            for dup_group in report['test_internal_duplicates']:
                # Garder le premier, supprimer les autres
                for file_path_str in dup_group['files'][1:]:
                    success, msg = delete_file(file_path_str)
                    if success:
                        count += 1
                        total_removed += 1
                    elif "introuvable" not in msg.lower():
                        print(f"   ‚ö†Ô∏è {Path(file_path_str).name}: {msg}")
            
            print(f"   ‚úÖ {count} fichiers supprim√©s")
        
        # 3. Supprimer les doublons internes du TRAIN
        if check_train_internal and remove_from in ['train', 'both']:
            print(f"\nüóëÔ∏è  Suppression des doublons internes du TRAIN...")
            
            count = 0
            for dup_group in report['train_internal_duplicates']:
                # Garder le premier, supprimer les autres
                for file_path_str in dup_group['files'][1:]:
                    success, msg = delete_file(file_path_str)
                    if success:
                        count += 1
                        total_removed += 1
                    elif "introuvable" not in msg.lower():
                        print(f"   ‚ö†Ô∏è {Path(file_path_str).name}: {msg}")
            
            print(f"   ‚úÖ {count} fichiers supprim√©s")
        
        print(f"\n‚úÖ TOTAL: {total_removed} fichiers supprim√©s avec succ√®s")
    
    # ============================================
    # √âTAPE 7: EXPORT DU RAPPORT (si demand√©)
    # ============================================
    if export_report:
        report_file = f"duplicates_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"\nüíæ Rapport export√©: {report_file}")
    
    print("\n" + "=" * 80)
    print("‚úÖ Analyse termin√©e!")
    print("=" * 80)
    
    return report

# Exemple d'utilisation:
# report = find_all_duplicates_exhaustive(
     train_dir, test_dir,
     check_train_internal=True ,
     check_test_internal=True ,
     check_cross= True,
     remove_duplicates= True ,  # Mettre True pour supprimer
     remove_from='test',  # 'test', 'train', ou 'both'
     export_report= True )


Ex√©cuter l'analyse exhaustive des doublons
============================================

Analyse compl√®te (sans suppression)
report = find_all_duplicates_exhaustive(
    train_dir, test_dir,
    check_train_internal=True,    # V√©rifier les doublons dans train
    check_test_internal=True,     # V√©rifier les doublons dans test
    check_cross=True,              # V√©rifier les doublons entre train et test
    remove_duplicates=True,       # Mettre True pour supprimer automatiquement
    remove_from='test',            # 'test', 'train', ou 'both'
    export_report=True             # Exporter un rapport JSON
)

Pour supprimer les doublons, modifiez remove_duplicates=True ci-dessus
ATTENTION: La suppression est irr√©versible!
